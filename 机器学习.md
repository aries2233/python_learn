# 机器学习

数据-->模型-->预测

## 数据集构成

特征值+目标值



## 算法分类

### 监督学习

目标值：类别 -->分类问题

目标值：连续型的数据 -->回归问题

### 无监督学习

目标值：无



## 开发流程

1）获取数据

2）数据处理

3）特征工程

4）机器学习算法训练 - 模型

5）模型评估

6）应用



## 学习框架

算法是核心，数据与计算是基础

找准定位

### 书籍推荐：

机器学习-“西瓜书”-周志华

统计学习方法-李航

深度学习-“花书”



## 特征工程

### 数据集

可用数据集

sklearn

kaggle

UCI



## sklearn   scikit-learn

sklearn.datasets

加载小规模数据集

datasets.load_*()

加载大规模数据集

datasets.fetch_*(data_home=None,subset='选择')

选择：train/test/all



#### 数据集返回值

datasets.basebBunch（字典格式）



### 数据集划分

训练70

测试30

sklearn.model_selection.train_test_split(arrays,*options)

​	x数据集的特征值

​	y数据集的特征值

​	test_size测试集大小，为float

​	random_state随机数种子

​	return 训练集特征值，测试集特征值，训练集目标值，测试集目标值



### 特征工程

数据和特征决定机器学习上限



（使用专业背景知识和技巧）

#### 特征抽取

​	机器学习算法--统计公示--数学公式

​	文本类型 转 数值

​	类型 转 数值



1将任意数据转换为数字特征

​	字典特征提取

​	文本特征提取

​	图像特征提取

2特征提取API

sklearn.feature_extraction



#### 字典特征提取

sklearn.feature_extraction.DictVectorizer(sparse=Ture...)

​	矩阵 二维数组

​	向量 一维数组

DictVectorizer.fit_transform()

返回sparse（稀疏）矩阵

​	将非零值 按位置表示出来

![1675790683349](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1675790683349.png)

![1675790716208](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1675790716208.png)

节约空间，提高效率



#### 文本特征提取

​	单词 作为 特征

方法一：sklearn.feature_extraction.text.CountVectorizer(stop_words=[])

stop_words停用词

​		CountVectorizer.fit_transform(对象)



### 步骤：

1、实例化一个转化器类

2、调用fit_transform



#### 中文文本特征提取

1.自动分词

jieba库

```python
import jieba

def cut_word(text):
    """
    进行中文分词
    """
    a=" ".join(list(jieba.cut(text)))
    return a

#text="我爱北京天安门"
#输出“我 爱 北京 天安门”
```

形成新数据

```python
data=["文本"]
data_new=[]
for sent in data:
    data_new.append(cut_word(sent))
```

2.与上述步骤一样



关键词：在某一类文章中频繁出现

方法2：TfidfVevtorizer

TF-IDF  衡量文章中词的重要程度

TF 词频

IDF 逆向文档频率

TF-IDF = TF * IDF

例：

​	1000篇文章--语料库

​	100篇文章--“非常”

​	10篇文章--“经济

​	文章A（100词）：10次“经济” TF-IDF=0.2

​	TF:10/100=0.1

​	IDF:log10 1000/10 = 2

​	文章B（100词）：10次“非常” TF-IDF=0.1

​	TF:10/100=0.1

​	IDF:log10 1000/100 = 1



sklearn.feature_extraction.text.TfidfVectorizer(stop_words=None)

TfidfVectorizer.fit_transform()



## 特征预处理

sklearn.preprocessing

### 归一化：无量纲化

把数据映射映射到[0,1]之间

![1675955754784](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1675955754784.png)

![1675956121233](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1675956121233.png)

sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)...)

MinMaxScalar.fit_transform(X)

X为numpy array格式的数据[n_samples,n_features]

返回值：形状相同的array



缺点：有异常值；鲁棒性较差



### 标准化

![1675956866743](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1675956866743.png)

标准差：集中程度

sklearn.preprocessing.StandardScaler()

StandardScaler.fit_transform(X)

X为numpy array格式的数据[n_samples,n_features]

返回值：形状相同的array



### 特征降维

ndarray

​	维数：嵌套的层数

​	0维 标量

​	1维 向量

​	2维 矩阵

​	n维

处理对象：二维数组

降维指降低特征的个数，得到一组”不相关“主变量的过程

效果：特征与特征之间不相关，减少冗余信息

#### 方法1：特征选择

在原有特征中找到主要特征

Filter过滤式

​	方差选择法：低方差特征过滤

​	相关系数--特征与特征之间的相关程度

Embeded嵌入式

​	决策树

​	正则化

​	深度学习



sklearn.feature_selection

低方差特征过滤

特征方差小：某个特征大多样本的值相近

特征方差大：某个特征很多样本的值有差别

sklearn.feature_selection.VarianceThreshold(threshold = 0.0)

​	删除所有低方差特征

​	Variance.fit_transform(X)

​	X为numpy array格式的数据[n_samples,n_features]

​	返回值：默认值删除所有样本中具有相同值得特征值

相关系数

皮尔逊相关系数

![1675959141338](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1675959141338.png)

```python
from scipy.stats import pearsonr

r =pearsonr(x,y)
```



特征与特征之间相关性很高：

​	1.选取其中一个

​	2.加权求和

​	3.主成分分析



### 方法2：主成分分析

定义：将高维数据转化为低维数据的过程，在此过程中可能舍弃原有数据，创造新的变量

作用：压缩数据维度，降低复杂度

应用：回归分析或聚类分析

将二维降到一维：找到合适的直线，通过矩阵运算得出主成分分析结果



sklearn.decomposition.PCA(n_components=None)

​	n_components

​		小数 保留百分之多少的信息

​		整数 减少到多少特征

​	将数据分解为较低维空间

PCA.fit_transform(X)

​	X为numpy array格式的数据[n_samples,n_features]

​	返回值：转换后指定维度的array



降维实战案例

[哔哩哔哩]: https://www.bilibili.com/video/BV1nt411r7tj?p=17&amp;vd_source=2a5b7ffa9cfbdd97fb8190ac53330baf	"降维实战案例"

合并表-》交叉表-》降维





# 分类算法

### sklearn转换器和估计器

转换器--特征工程的父类

1.实例化

2.调用fit_transform

fit()

计算 每一列的平均值、标准差

transform()

(x - mean)/std 进行最终转换



估计器--sklearn机器学习算法的实现

1.实例化一个estimator

2.estimator.fit(x_train,y_train)计算

​		调用完毕，模型生成

3.模型评估

![1676036844262](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676036844262.png)

#比对真实值和预测值

y_predict = estimator.predict(x_test)

print("y_predict:\n",y_predict)

print("直接对比真实值和预测值:\n",y_test == y_predict)

#计算准确率

score = estimator.score(x_test,y_test)

print("准确率为:\n",score)

### KNN算法

根据邻居推断类别

定义：一个样本在特征空间中的K个最相似的样本中的大多数属于一个类别，则该样本也属于这个类别

#### 距离公式

欧式距离

![1676037218840](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676037218840.png)

曼哈顿距离 绝对值距离

明科夫斯基距离



实例：电影类型分析

![1676037352972](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676037352972.png)

​	k=1 爱情片

​	k=2 爱情片

​	k=...

k值取得过小，容易受到异常点影响

k值取得过大，样本不均衡影响



KNN算法的无量纲化处理：标准化

sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')

​	n_neighbors：int，默认5

​	algorithm：{'auto','ball_tree','kd_tree','brute'}



优点：简单易理解

缺点：对k的取值，计算量大，内存消耗多



### 模型选择与调优

交叉验证：将训练集分为训练和验证集。

![1676040609848](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676040609848.png)

四折交叉验证



超参数搜索--网格搜索

​	k的取值(1,3,5,7,9,11)

​	暴力破解



sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None)

​	estimator：估计器对象

​	param_grid：估计器参数(dict){"n_neighbors":[1,3,5]}

​	cv: 指定几折交叉验证（10折）

​	fit():输入训练数据

​	score(): 准确率

结果分析：

​	最佳参数：best_params_

​	最佳结果：best_score_

​	最佳预估器：best_estimator_

​	交叉验证结果：cv_results_





## 朴素贝叶斯算法

根据对象类容构成比例进行分类

概率：P(X)取[0,1]

联合概率：多个条件，同时成立

条件概率：在一条件下发生的概率

相互独立：P(AB)=P(A)P(B)



贝叶斯公式

### ![1676107513058](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676107513058.png)



朴素：假设特征与特征之间相互独立

应用场景：文本分类、单词作为特征



朴素贝叶斯算法对文本分类

![1676107993422](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676107993422.png)

拉普拉斯平滑系数

![1676108085459](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676108085459.png)



sklearn.naive_bayes.MultinomialNB(alpha=1.0)

​	alpha拉普拉斯平滑系数



优点：稳定，简单，对缺失数据不敏感，常用于文本分类，准确度高速度快

缺点：使用了独立性假设，若特征属性有关联则效果不好

案例：20类新闻分类



### 决策树

if-else结构分割数据

how高效：特征的先后顺序

### 信息论基础

香农：消除随机不定性的东西

信息的衡量--信息量--信息熵

![1676132165213](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676132165213.png)

bit

决策树划分依据之一——信息增益

![1676132600697](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676132600697.png)

![1676132618808](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676132618808.png)

classsklearn.tree.DecisionTreeClass(criterion='gini',max_depth=None,random_state=None)

​	决策树分类器

​	criterion：默认'gini'系数，也可以是信息增益的熵'entropy'

​	max_depth：树的深度大小

​	random_state：随机数种子 



#### 决策树可视化http://webgraphviz.com/

sklearn.tree.export_graphviz()

tree.export_graphviz(estimator,out_file='tree.dot',feature_names=[","])



优点：简单，可视化，解释能力强

缺点：容易产生过拟合（方法：减枝，随机森林）



## 回归于聚类

### 线性回归

回归问题

目标值——连续型数据

定义：利用函数对一个或多个特征值和目标值之间的关系进行建模分析

线性模型：

![1676293314383](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676293314383.png)

![1676293418682](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676293418682.png)

广义线性模型

​	线性关系——呈直线关系

​	非线性关系?



​	线性模型

​		自变量（特征值）一次

​			y = w1x1 + w2x2 + w3x3 + ... + wnxn + b

​		参数一次

​			y = w1x1 + w2x1 ^ 2 + w3x1^3+ w4x2^3 + ... + b

线性关系一定是线性模型

线性模型不一定是线性关系



线性回归的损失和优化原理

目标：求模型参数

​	模型参数能够使得预测准确

损失函数/cost/成本函数/目标函数

![1676302588264](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676302588264.png)

### 优化损失

正规方程

​	天才——直接求解w

![1676302686224](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676302686224.png)

拓展：

1）

​	y = ax^2 + bx + c

​	y' = 2ax + b = 0

​	x = -b / 2a

2）

​	a * b = 1

​		b = 1 / a = a^-1

​	A * B = E

[[1,0,0],

[0,1,0],

[0,0,1]]

​	B = A^-1

缺点：当特征多且复杂时，求解速度慢或得不到结果



梯度下降

​	勤奋努力的普通人

​		试错、改进'

![1676305480335](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676305480335.png)

迭代

![1676305498297](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676305498297.png)

对于非线性回归有特殊情况



API

sklearn.linear_model.LinearRegression(fit_intercept=True)

​	通过正规方程优化

​	fit_intercept是否计算偏置

​	LinearRegression.coef_回归系数

​	LinearRegression.intercept_偏置

sklearn.linear_model.SGDRegressor(loss="squared_loss",fit_intercept=True,learning_rate='invscaling',eta0=0.01)

​	实现随机梯度下降学习，支持不同的loss函数和正则化惩罚项来拟合线性回归模型。

​	loss：损失类型"squared_loss"普通最小二乘法

​	fit_intercept:是否计算偏差

​	learning_rate学习率:

​		'constant':eta=eta0

​		'optimal':eta=1.0/(alpha*(t+t0))[default]

​		'invscaling':eta=eta0/pow(t,power_t)

power_t=0.25



SGDRegressor.coef_：回归系数

SGDRegressor.intercept_：偏置



案例：波士顿房价预测



### 回归性能评估

![1676307973456](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676307973456.png)

sklearn.metrics.mean_squared_error(y_true,y_pred)

​	 均方误差回归损失

​	y_true真实值

​	y_pred预测值

​	return 浮点数结果

对比

![1676308491085](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676308491085.png)



梯度下降优化器

GD

原始算法，计算所有样本的值，计算量大

SGD

一次迭代只考虑一个样本

优点：高效

缺点：需要许多超参数，对特征标准化敏感

SAG（随机平均梯度）




