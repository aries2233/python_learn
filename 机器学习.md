# 机器学习

数据-->模型-->预测

## 数据集构成

特征值+目标值



## 算法分类

### 监督学习

目标值：类别 -->分类问题

目标值：连续型的数据 -->回归问题

### 无监督学习

目标值：无



## 开发流程

1）获取数据

2）数据处理

3）特征工程

4）机器学习算法训练 - 模型

5）模型评估

6）应用



## 学习框架

算法是核心，数据与计算是基础

找准定位

### 书籍推荐：

机器学习-“西瓜书”-周志华

统计学习方法-李航

深度学习-“花书”



## 特征工程

### 数据集

可用数据集

sklearn

kaggle

UCI



## sklearn   scikit-learn

sklearn.datasets

加载小规模数据集

datasets.load_*()

加载大规模数据集

datasets.fetch_*(data_home=None,subset='选择')

选择：train/test/all



#### 数据集返回值

datasets.basebBunch（字典格式）



### 数据集划分

训练70

测试30

sklearn.model_selection.train_test_split(arrays,*options)

​	x数据集的特征值

​	y数据集的特征值

​	test_size测试集大小，为float

​	random_state随机数种子

​	return 训练集特征值，测试集特征值，训练集目标值，测试集目标值



### 特征工程

数据和特征决定机器学习上限



（使用专业背景知识和技巧）

#### 特征抽取

​	机器学习算法--统计公示--数学公式

​	文本类型 转 数值

​	类型 转 数值



1将任意数据转换为数字特征

​	字典特征提取

​	文本特征提取

​	图像特征提取

2特征提取API

sklearn.feature_extraction



#### 字典特征提取

sklearn.feature_extraction.DictVectorizer(sparse=Ture...)

​	矩阵 二维数组

​	向量 一维数组

DictVectorizer.fit_transform()

返回sparse（稀疏）矩阵

​	将非零值 按位置表示出来

![1675790683349](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1675790683349.png)

![1675790716208](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1675790716208.png)

节约空间，提高效率



#### 文本特征提取

​	单词 作为 特征

方法一：sklearn.feature_extraction.text.CountVectorizer(stop_words=[])

stop_words停用词

​		CountVectorizer.fit_transform(对象)



### 步骤：

1、实例化一个转化器类

2、调用fit_transform



#### 中文文本特征提取

1.自动分词

jieba库

```python
import jieba

def cut_word(text):
    """
    进行中文分词
    """
    a=" ".join(list(jieba.cut(text)))
    return a

#text="我爱北京天安门"
#输出“我 爱 北京 天安门”
```

形成新数据

```python
data=["文本"]
data_new=[]
for sent in data:
    data_new.append(cut_word(sent))
```

2.与上述步骤一样



关键词：在某一类文章中频繁出现

方法2：TfidfVevtorizer

TF-IDF  衡量文章中词的重要程度

TF 词频

IDF 逆向文档频率

TF-IDF = TF * IDF

例：

​	1000篇文章--语料库

​	100篇文章--“非常”

​	10篇文章--“经济

​	文章A（100词）：10次“经济” TF-IDF=0.2

​	TF:10/100=0.1

​	IDF:log10 1000/10 = 2

​	文章B（100词）：10次“非常” TF-IDF=0.1

​	TF:10/100=0.1

​	IDF:log10 1000/100 = 1



sklearn.feature_extraction.text.TfidfVectorizer(stop_words=None)

TfidfVectorizer.fit_transform()



## 特征预处理

sklearn.preprocessing

### 归一化：无量纲化

把数据映射映射到[0,1]之间

![1675955754784](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1675955754784.png)

![1675956121233](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1675956121233.png)

sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)...)

MinMaxScalar.fit_transform(X)

X为numpy array格式的数据[n_samples,n_features]

返回值：形状相同的array



缺点：有异常值；鲁棒性较差



### 标准化

![1675956866743](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1675956866743.png)

标准差：集中程度

sklearn.preprocessing.StandardScaler()

StandardScaler.fit_transform(X)

X为numpy array格式的数据[n_samples,n_features]

返回值：形状相同的array



### 特征降维

ndarray

​	维数：嵌套的层数

​	0维 标量

​	1维 向量

​	2维 矩阵

​	n维

处理对象：二维数组

降维指降低特征的个数，得到一组”不相关“主变量的过程

效果：特征与特征之间不相关，减少冗余信息

#### 方法1：特征选择

在原有特征中找到主要特征

Filter过滤式

​	方差选择法：低方差特征过滤

​	相关系数--特征与特征之间的相关程度

Embeded嵌入式

​	决策树

​	正则化

​	深度学习



sklearn.feature_selection

低方差特征过滤

特征方差小：某个特征大多样本的值相近

特征方差大：某个特征很多样本的值有差别

sklearn.feature_selection.VarianceThreshold(threshold = 0.0)

​	删除所有低方差特征

​	Variance.fit_transform(X)

​	X为numpy array格式的数据[n_samples,n_features]

​	返回值：默认值删除所有样本中具有相同值得特征值

相关系数

皮尔逊相关系数

![1675959141338](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1675959141338.png)

```python
from scipy.stats import pearsonr

r =pearsonr(x,y)
```



特征与特征之间相关性很高：

​	1.选取其中一个

​	2.加权求和

​	3.主成分分析



### 方法2：主成分分析

定义：将高维数据转化为低维数据的过程，在此过程中可能舍弃原有数据，创造新的变量

作用：压缩数据维度，降低复杂度

应用：回归分析或聚类分析

将二维降到一维：找到合适的直线，通过矩阵运算得出主成分分析结果



sklearn.decomposition.PCA(n_components=None)

​	n_components

​		小数 保留百分之多少的信息

​		整数 减少到多少特征

​	将数据分解为较低维空间

PCA.fit_transform(X)

​	X为numpy array格式的数据[n_samples,n_features]

​	返回值：转换后指定维度的array



降维实战案例

[哔哩哔哩]: https://www.bilibili.com/video/BV1nt411r7tj?p=17&amp;vd_source=2a5b7ffa9cfbdd97fb8190ac53330baf	"降维实战案例"

合并表-》交叉表-》降维





# 分类算法

### sklearn转换器和估计器

转换器--特征工程的父类

1.实例化

2.调用fit_transform

fit()

计算 每一列的平均值、标准差

transform()

(x - mean)/std 进行最终转换



估计器--sklearn机器学习算法的实现

1.实例化一个estimator

2.estimator.fit(x_train,y_train)计算

​		调用完毕，模型生成

3.模型评估

![1676036844262](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676036844262.png)

#比对真实值和预测值

y_predict = estimator.predict(x_test)

print("y_predict:\n",y_predict)

print("直接对比真实值和预测值:\n",y_test == y_predict)

#计算准确率

score = estimator.score(x_test,y_test)

print("准确率为:\n",score)

### KNN算法

根据邻居推断类别

定义：一个样本在特征空间中的K个最相似的样本中的大多数属于一个类别，则该样本也属于这个类别

#### 距离公式

欧式距离

![1676037218840](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676037218840.png)

曼哈顿距离 绝对值距离

明科夫斯基距离



实例：电影类型分析

![1676037352972](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676037352972.png)

​	k=1 爱情片

​	k=2 爱情片

​	k=...

k值取得过小，容易受到异常点影响

k值取得过大，样本不均衡影响



KNN算法的无量纲化处理：标准化

sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')

​	n_neighbors：int，默认5

​	algorithm：{'auto','ball_tree','kd_tree','brute'}



优点：简单易理解

缺点：对k的取值，计算量大，内存消耗多



### 模型选择与调优

交叉验证：将训练集分为训练和验证集。

![1676040609848](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676040609848.png)

四折交叉验证



超参数搜索--网格搜索

​	k的取值(1,3,5,7,9,11)

​	暴力破解



sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None)

​	estimator：估计器对象

​	param_grid：估计器参数(dict){"n_neighbors":[1,3,5]}

​	cv: 指定几折交叉验证（10折）

​	fit():输入训练数据

​	score(): 准确率

结果分析：

​	最佳参数：best_params_

​	最佳结果：best_score_

​	最佳预估器：best_estimator_

​	交叉验证结果：cv_results_





## 朴素贝叶斯算法

根据对象类容构成比例进行分类

概率：P(X)取[0,1]

联合概率：多个条件，同时成立

条件概率：在一条件下发生的概率

相互独立：P(AB)=P(A)P(B)



贝叶斯公式

### ![1676107513058](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676107513058.png)



朴素：假设特征与特征之间相互独立

应用场景：文本分类、单词作为特征



朴素贝叶斯算法对文本分类

![1676107993422](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676107993422.png)

拉普拉斯平滑系数

![1676108085459](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676108085459.png)



sklearn.naive_bayes.MultinomialNB(alpha=1.0)

​	alpha拉普拉斯平滑系数



优点：稳定，简单，对缺失数据不敏感，常用于文本分类，准确度高速度快

缺点：使用了独立性假设，若特征属性有关联则效果不好

案例：20类新闻分类



### 决策树

if-else结构分割数据

how高效：特征的先后顺序

### 信息论基础

香农：消除随机不定性的东西

信息的衡量--信息量--信息熵

![1676132165213](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676132165213.png)

bit

决策树划分依据之一——信息增益

![1676132600697](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676132600697.png)

![1676132618808](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676132618808.png)

classsklearn.tree.DecisionTreeClass(criterion='gini',max_depth=None,random_state=None)

​	决策树分类器

​	criterion：默认'gini'系数，也可以是信息增益的熵'entropy'

​	max_depth：树的深度大小

​	random_state：随机数种子 



#### 决策树可视化http://webgraphviz.com/

sklearn.tree.export_graphviz()

tree.export_graphviz(estimator,out_file='tree.dot',feature_names=[","])



优点：简单，可视化，解释能力强

缺点：容易产生过拟合（方法：减枝，随机森林）



## 回归于聚类

### 线性回归

回归问题

目标值——连续型数据

定义：利用函数对一个或多个特征值和目标值之间的关系进行建模分析

线性模型：

![1676293314383](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676293314383.png)

![1676293418682](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676293418682.png)

广义线性模型

​	线性关系——呈直线关系

​	非线性关系?



​	线性模型

​		自变量（特征值）一次

​			y = w1x1 + w2x2 + w3x3 + ... + wnxn + b

​		参数一次

​			y = w1x1 + w2x1 ^ 2 + w3x1^3+ w4x2^3 + ... + b

线性关系一定是线性模型

线性模型不一定是线性关系



线性回归的损失和优化原理

目标：求模型参数

​	模型参数能够使得预测准确

损失函数/cost/成本函数/目标函数

![1676302588264](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676302588264.png)

### 优化损失

正规方程

​	天才——直接求解w

![1676302686224](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676302686224.png)

拓展：

1）

​	y = ax^2 + bx + c

​	y' = 2ax + b = 0

​	x = -b / 2a

2）

​	a * b = 1

​		b = 1 / a = a^-1

​	A * B = E

[[1,0,0],

[0,1,0],

[0,0,1]]

​	B = A^-1

缺点：当特征多且复杂时，求解速度慢或得不到结果



梯度下降

​	勤奋努力的普通人

​		试错、改进'

![1676305480335](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676305480335.png)

迭代

![1676305498297](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676305498297.png)

对于非线性回归有特殊情况



API

sklearn.linear_model.LinearRegression(fit_intercept=True)

​	通过正规方程优化

​	fit_intercept是否计算偏置

​	LinearRegression.coef_回归系数

​	LinearRegression.intercept_偏置

sklearn.linear_model.SGDRegressor(loss="squared_loss",fit_intercept=True,learning_rate='invscaling',eta0=0.01)

​	实现随机梯度下降学习，支持不同的loss函数和正则化惩罚项来拟合线性回归模型。

​	loss：损失类型"squared_loss"普通最小二乘法

​	fit_intercept:是否计算偏差

​	learning_rate学习率:

​		'constant':eta=eta0

​		'optimal':eta=1.0/(alpha*(t+t0))[default]

​		'invscaling':eta=eta0/pow(t,power_t)

power_t=0.25



SGDRegressor.coef_：回归系数

SGDRegressor.intercept_：偏置



案例：波士顿房价预测



### 回归性能评估

![1676307973456](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676307973456.png)

sklearn.metrics.mean_squared_error(y_true,y_pred)

​	 均方误差回归损失

​	y_true真实值

​	y_pred预测值

​	return 浮点数结果

对比

![1676308491085](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676308491085.png)



梯度下降优化器

GD

原始算法，计算所有样本的值，计算量大

SGD

一次迭代只考虑一个样本

优点：高效

缺点：需要许多超参数，对特征标准化敏感

SAG（随机平均梯度）



### 欠拟合与过拟合

问题：训练集表现很好，测试集不好——过拟合

欠拟合（模型过于简单）

​	增加数据的特征数量

过拟合（模型过于复杂）

​	正则化



#### L2正则化 Ridge

​	使某些W都接近于0

​	损失函数 + 惩罚项![1676364192837](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676364192837.png)



#### L1正则化 LASSO

​	使某些W直接为0

​	损失函数 + 惩罚项



### Ridge岭回归

sklearn.linear_model.Ridge(alpha=1.0,fit_intercept=True,solver="auto",normalize=False)

alpha正则化力度0-1,1-10

solver：自动选择优化方法

​	sag如果数据集、特征较大，选择该随机梯度下降优化

normalize数据是否进行标准化

​	normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据

Ridge.coef_回归权重

Ridge.intercept_回归权重



sklearn.linear_model.RidgeCV(_BaseRidgeCV,RegressorMixin)

​	l2正则化的线性回归，可进行交叉验证

​	coef——回归系数

正则化力度越大，权重系数越小

正则化力度越小，权重系数越大



### 逻辑回归与二分类

分类算法

逻辑回归是解决二分类问题的利器

![1676386184877](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676386184877.png)

线性回归的输出 就是 逻辑回归的输入

激活函数

​	sigmoid函数

![1676386230150](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676386230150.png)

​	1/(1+e^(-x))

输出结果：[0,1]区间中的一个概率值，默认阈值0.5

假设函数/线性模型

​	1/(1+e^(w1x1 + w2x2 + w3x3 + ... + wnxn + b))

损失函数

​	(y_predict - y_true)平方和/总数

​	逻辑回归的真实值/预测值 是否属于某个类别

​	对数似然损失

![1676386497845](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676386497845.png)

![1676386600081](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676386600081.png)

![1676386570041](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676386570041.png)

![1676386678535](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676386678535.png)

优化损失



sklearn.linear_model.LogisticRegression(solver='liblinear',penalty='12',C=1.0)

​	solver优化求解方式

​		sag:根据数据集自动选择，随机平均梯度下降

​	penalty:正则化的种类

​	C:正则化力度



LogisticRegression方法相当于SGDClassifier(loss="log",penalty=""),SGDClassifier实现了一个普通的随机梯度下降学习，也支持平均随机梯度下降法（ASGD），可以设置average=True.而使用LogisticRegression(实现了SAG)



案例：癌症分类



### 精确率和召回率

混淆矩阵

![1676395605068](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676395605068.png)

TP = True Possitive

FN = False Negative

精确率 TP/(TP+FP)

召回率TP/(TP+FN)

F1-score模型的稳健性

![1676395872842](C:\Users\Wang-Sir\AppData\Roaming\Typora\typora-user-images\1676395872842.png)



分类评估报告

sklearn.metrics.classification_report(y_true,y_pred,labels=[],target_names=None)

​	y_true:真实目标值

​	y_pred:预估器预测目标值

​	labels:指定类别对应的数字

​	target_names:目标类别名称

​	return:每个类别精确率与召回率



print("精确率和召回率:\n",classification_report(y_test,lr.predict(x_test)))



recall召回率



### ROC曲线与AUC指标

TPR = TP / (TP+FN) —— 召回率

所有真实类别为1的样本，预测类别为1的比例

FDR = FP / (FP + TN)

所有真实类别为0的样本，预测类别为1的比例

ROC曲线

AUC指标

AUC越接近1越完美



sklearn.metrics.roc_auc_score(y_true,y_score)

​	计算ROC曲线面积，即AUC值

​	y_true:每个样本的真实案例，必须为0（反例），1（正例）标记

​	y_score:预测得分，可以是正类的估计概率、置信值或分类器方法的返回值

AUC只能用于评价二分类



模型保存和加载

sklearn.externals.joblib

保存joblib.dump(rf,"test.pkl")

加载estimator=joblib.load("test.pkl")



## 无监督学习-K-means算法

没有目标值--无监督学习

聚类K-means（K均值聚类）

降维PCA



步骤：

随机设置K个特征空间内的点作为初始的聚类中心

1）看需求

2）调节超参数（网格搜索）



sklearn.cluster.KMeans(n_clusters=8,init='k-means++')

​	k-means聚类

​	n_clusters开始的聚类中心数量

​	init初始化方法 默认为

​	labels_:默认标记类型，可以与真实值比较



Kmeans性能评估指标

轮廓系数

”高内聚，低耦合“

内部距离最小化

外部距离最大化

如果b_i>>a_i趋近于1效果越好

b_i<<a_i趋近于-1效果不好

轮廓系数介于[-1,1]

趋近于1代表内聚度和分离度都相对较优

sklearn.metrics.silhouette_score(X,labels)

​	计算样本的平均轮廓系数

​	X特征值

​	labels:被聚类标记的目标值

特点：采用迭代算法

缺点：容易收敛到局部最优解（多次聚类）

